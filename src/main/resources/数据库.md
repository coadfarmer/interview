# MYSQL

### 数据库三大范式

1. 第一范式：每个列不可拆分
2. 第二范式：在第一范式的基础上，非主键列都完全依赖主键，而不是依赖主键的一部分
3. 第三范式：在第二范式的基础上，非主键列只依赖于主键，不依赖于其他非主键

### MYSQL有哪些常用数据类型

1. 整数类型

   包括TINYINT、SMALLINT、MEDIUMINT、INT、BIGINT，分别表示1字节、2字节、3字节、4字节、8字节整数。任何整数类型都可以加上UNSIGNED属性，表示数据是无符号的，即非负整数。

2. 实数类型

   包括FLOAT、DOUBLE、DECIMAL（小数精确）。

3. 字符串类型

   char（定长）varchar（不定长）

   **注意**：

   对于经常变更的数据来说，CHAR比VARCHAR更好，因为CHAR不容易产生碎片。 对于非常短的列，CHAR比VARCHAR在存储空间上更有效率。 使用时要注意只分配需要的空间，更长的列排序时会消耗更多内存。
   尽量避免使用TEXT/BLOB类型，查询时会使用临时表，导致严重的性能开销。

4. 枚举类型：ENUM

5. 时间类型

   timestamp/datetime

### B+树

![B+tree](http://imysql.com/wp-content/uploads/2014/09/B-tree.png)

B+树，和B树一样，也是一种平衡的多叉查找树，不同之处在于：

- B树内部节点保存数据而B+树内部节点不保存数据，只做索引作用，它的叶子节点才保存数据
- B+树相邻的叶子节点之间通过链表指针连接起来

### MyISAM和InnoDB的区别

- myisam是非聚簇索引，innodb是聚簇索引
- MyISAM叶子节点存储的是数据地址，需要再寻址一次才能查询到数据
- Innodb的主键索引的叶子节点存储行数据，因此主键索引非常高效
- InnoDB非主键索引的叶子节点存储的是主键和其他索引的列数据，因此查询时做到覆盖索引会非常高效

### 什么是索引

通俗地说，索引就是目录。在数据库系统中，索引是一种排好序的数据结构，它包含数据表里所有记录的指针。索引的实现通常是B+树。

### 索引有哪些优缺点

- 优点
    - 可以大大加快数据的检索速度
- 缺点
    - 在增删改的时候，索引需要动态维护，降低了增删改的执行效率
    - 索引需要占用物理空间

### 索引使用场景

- where
- order by
- group by
- join
- distinct

### 索引有哪几种类型

- 主键索引
- 唯一索引
- 普通索引
- 全文索引

### 索引的算法和数据结构

B+树和hash，相对应的是B+树算法和hash算法。

B+树是一种多叉平衡树，数据保存在叶子中（B树是将信息保存在所有节点中），并且叶子节点上都有两个指针，分别指向上一个节点和下一个节点，组成一个双向链表。（一个节点就是1页，innoDB引擎存储是以页为单位的）

Hash算法，Hash碰撞的数据组成一个链表

### 创建索引的原则

1. 索引不是越多越好：虽然索引可以提高查询速度，但是过多的索引会增加存储空间和维护成本，甚至会降低更新、插入和删除的性能。因此，应该根据实际需求，有选择地创建索引。
2. 索引应该选择高选择性的列：选择性越高的列，区分度就越大，能够更快地过滤不符合条件的数据，因此创建索引的效果也就越好。例如，一个有大量重复值的列不适合作为索引列。
3. 考虑多列索引：多列索引可以提高查询的效率，但需要根据实际情况选择合适的索引组合，不能随意组合。
4. 考虑索引的顺序：在创建多列索引时，索引列的顺序对查询效率有影响。一般情况下，选择选择性高的列放在前面。
5. 对于经常进行排序、分组、连接操作的列，可以考虑创建索引。
6. 对于小表，不需要创建索引。因为小表通常可以在内存中快速执行查询操作。
7. 避免使用过长的索引列。索引列过长会占用更多的存储空间，降低查询效率。
8. 考虑删除不需要的索引。不需要的索引不仅占用存储空间，还会影响插入、更新和删除的性能。

### 事务的四大特性

1. 原子性（Atomicity）：事务是最小的执行单位，要么全成功，要么全失败；
2. 一致性（Consistency）：事务执行前后，数据库中的数据应该保持一致性状态。如果事务执行过程中出现错误，所有已经执行的操作都会回滚，数据恢复到事务开始之前的状态。
3. 隔离性（Isolation）：多个事务并发执行时，每个事务都应该感觉不到其他事务的存在，每个事务都有独立的空间和资源，互不干扰。事务隔离级别越高，隔离性越好，但并发性能越差。
4. 持久性（Durability）：一旦事务提交，数据就会被永久保存到数据库中，即使系统崩溃也不会丢失。

### 什么是脏读，不可重复读，幻读

- 脏读：当一个事务正在访问数据并且对数据进行了修改，但是尚未提交时，另外一个事务也访问了这个数据，此时第二个事务读取到的是尚未提交的数据，这种现象被称为脏读。
- 不可重复读：当一个事务正在访问并读取数据时，另一个事务也访问了这个数据并进行了修改并提交，此时第一个事务再次读取到相同的数据，结果却不同，这种现象被称为不可重复读。
- 幻读：当一个事务在读取一定范围内的数据时，另外一个事务在该范围内插入了新的数据，第一个事务再次读取该范围内的数据时，发现数据比第一次读取的结果多了几条，这种现象被称为幻读。

### 什么是事务的隔离级别

| 隔离级别                     | 脏读 | 不可重复读 | 幻读 |
| ---------------------------- | ---- | ---------- | ---- |
| READ-UNCOMMITTED（读未提交） | √    | √          | √    |
| READ-COMMITTED（读已提交）   | ×    | √          | √    |
| REPEATABLE-READ（可重复读）  | ×    | ×          | √    |
| SERIALIZABLE（串行化）       | ×    | ×          | ×    |

MySQL默认采用REPEATABLE-READ（可重复读）隔离级别，Oracle默认采用READ_COMMITTED隔离级别

事务隔离机制基于锁机制和并发调度，并发调度使用的是MVVC

### MYSQL锁机制

#### 隔离级别与锁的关系

| 隔离级别                     | 锁                                           |
| ---------------------------- | -------------------------------------------- |
| READ-UNCOMMITTED（读未提交） | 读数据不加共享锁                             |
| READ-COMMITTED（读已提交）   | 读数据加共享锁，读完后释放共享锁             |
| REPEATABLE-READ（可重复读）  | 读数据加共享锁，事务提交后释放共享锁         |
| SERIALIZABLE（串行化）       | 锁定整个范围的键，并一直持有锁，直到事务完成 |

#### 按照锁粒度分为哪些锁

- 行级锁（InnoDB默认）
- 表级锁
- 页级锁

#### 按照锁类别分为哪些锁

- 共享锁：也就是读锁，用户读数据时，对数据加上共享锁
- 排他锁：写锁

#### 会发生死锁吗？怎么解决

会，当两个或多个事务在同一资源上相互占用，并请求锁定对方的资源，就会造成死锁。

解决方法：

1. 如果不同程序会并发存取多个表，尽量以相同的顺序访问表。
2. 在同一事务中，尽量做到一次锁定所需要的所有资源，减少死锁产生的概率。
3. 可以尝试升级锁粒度
4. 可以尝试使用分布式事务锁或乐观锁

#### MYSQL关联查询类型

1. 内连接
2. 外连接
3. 联合查询（union）：联合查询结果是将多个select语句的查询结果联合到一起。
4. 笛卡尔查询（cross join）

### EXPLAIN命令

- select_type:查询类型，simple、primary、union等
- **type**：访问类型：all，index，range，ref，eq_ref，const，system，NULL 越后面性能越好，《阿里巴巴Java开发手册》中推荐至少为range级别（索引范围查找）
- key：实际使用的索引
- extra：using index，using where

### 其他优化命令

- ANALYZE：分析表，如ANALYZE TABLE user;

- OPTIMIZE：优化表，如OPTIMIZE [LOCAL|NO_WRITE_TO_BINLOG] TABLE user;

  LOCAL|NO_WRITE_TO_BINLOG都是表示不写入日志.,优化表只对VARCHAR,BLOB和TEXT有效,通过OPTIMIZE TABLE语句可以消除文件碎片,在执行过程中会加上只读锁.

### 大表数据查询，怎么优化

1. 加索引
2. 加缓存
3. 读写分离
4. 分库分表

> [面对MySQL千万级别大表，你要如何优化？ | HeapDump性能社区](https://heapdump.cn/article/3850295)

#### 超大分页怎么处理

1. sql语句层面，比如取一百万行后的10条数据，mysql实际操作会取一百零十万行，然后截掉前100万行。这样会非常影响性能。

   比如一条语句 select * from table where 条件 limit 1000000,10;可以改为：

   select * from table where id in (select id from table where 条件 limit 1000000,10);

   更好的写法为：

   select * from table a,(select id from table where 条件 limit 100000,10) b where a.id = b.id

2. 加缓存

### 谈一谈数据库优化经验

- #### SQL语句层面

    - 对查询进行优化，尽量避免全表扫描，首先考虑在where，order by涉及的字段建立索引
    - 在where子句中使用null值判断、!=或<>操作符、or连接条件、in 和 not in、左%匹配、参数、字段的表达式操作、函数操作都将导致全表扫描，影响性能

- #### 数据库层面

    - 将字段很多的表分解为多张表
    - 增加冗余字段

### 什么是MVCC

MVCC全称是Multi-Version Concurrency Control（多版本并发控制），是数据库管理系统中用于实现并发控制的一种机制。它基于对数据的版本进行管理来实现对并发事务的控制，是一种在读写锁机制的基础上实现高并发控制的技术。

在MVCC机制中，每个事务在进行读写操作时，会根据时间戳或者版本号等方式获取到当前的数据版本，如果当前事务修改的数据版本已经被其他事务修改，则当前事务需要等待其他事务释放该数据版本的锁。

同时，MVCC还支持快照读取，即读取某个时间点或者版本号的数据快照，避免了读取到脏数据的问题。由于MVCC机制只需要锁定每个事务的读写版本，而不需要锁定整个表或者行，因此可以大大提高并发度和性能。

### MYSQL日志

MYSQL日志主要包括错误日志，查询日志，慢查询日志，事务日志，二进制日志等几大类。其中比较重要的是binlog（二进制日志）、redo log（事务日志）和undo log（回滚日志）。

### 数据库三大日志

1. **binlog**（二进制日志）：记录对数据库的更新操作，例如增删改表，增删改数据等。记录的是SQL语句的二进制格式，可以用来实现数据的复制、恢复以及灾备等功能。
2. **redo log**（重做日志）：记录每个数据页在磁盘上的修改操作，包括物理页号、修改的偏移量、修改前和修改后的值等。主要是为了恢复数据库在发生异常（如宕机）时，未来得及写入到磁盘的数据。MySQL使用循环写的方式记录redo log，当redo log写满后，会回到开头重新写，覆盖之前的日志。
3. **undo log**（回滚日志）：记录事务中发生的修改操作，以便在回滚事务时撤销这些操作。undo log在事务提交时，会把修改操作应用到磁盘数据页上。在事务回滚时，会读取undo log，将之前修改的数据页还原回去。

# Redis

### 简单介绍下redis

Redis是一个由C语言开发的数据库，Redis的数据是存在内存中的，所以读写速度非常快。

Redis一般被用作缓存数据库，分布式锁，消息队列。

Redis的所有操作都是原子性的

Redis支持事务，持久化，集群。

### Redis除了做缓存，还能做什么

- 分布式锁——SETNX、Redisson
- 计数器：Redis的INCR/DECR命令可以实现计数器功能，可以用来实现网站的访问量统计、用户粉丝数量等计数功能。
- 消息队列
- 排行榜：利用Redis的有序集合和计数器功能，可以实现排行榜的实时更新和查询。

### Redis分布式锁有哪几种类型？

### Redis数据类型

Redis支持五大数据类型：String，Hash，List，Set，ZSet

- String：Redis最基本的数据类型，一个key对应一个value，最大存储512MB。

- Hash：一个key对应一个HashMap

- List：一个key对应一个List

- Set：一个key对应一个Set，通过Hash表实现

- ZSet（Sorted Set）：和Set的区别是ZSet是一个排好序的Set

  ```bash
  redis 127.0.0.1:6379> ZADD runoobkey 1 redis
  (integer) 1
  redis 127.0.0.1:6379> ZADD runoobkey 2 mongodb
  (integer) 1
  redis 127.0.0.1:6379> ZADD runoobkey 3 mysql
  (integer) 1
  redis 127.0.0.1:6379> ZADD runoobkey 3 mysql
  (integer) 0
  redis 127.0.0.1:6379> ZADD runoobkey 4 mysql
  (integer) 0
  redis 127.0.0.1:6379> ZRANGE runoobkey 0 10 WITHSCORES
  
  1) "redis"
  2) "1"
  3) "mongodb"
  4) "2"
  5) "mysql"
  6) "4"
  ```

- bitmap：位图，占空间小，只有0和1

### Redis持久化

#### 什么是Redis持久化

Redis持久化就是把内存中的数据写入磁盘中去

#### Redis持久化的机制是什么？有什么优缺点

Redis提供两种持久化机制：RDB和AOF机制

- RDB（Redis DataBase）

  RDB是Redis默认的持久化方式，按照一定的时间将内存中的数据以快照的形式保存到磁盘中，对应产生的数据文件是dump.rdb。通过配置文件中的sava参数来定义快照周期。

  优点：

    - 只有一个文件，方便
    - 性能好

  缺点：

    - 安全性低

- AOF（Append only file）

  每执行一条会更改Redis中数据的命令，Redis就会将该命令写入到内存缓存中（server.aof_buf）中，然后根据appendsync配置来决定何时持久化到硬盘文件（appendonly.aof）中。appendsync有三种同步方式：

    - always：每一条命令都同步
    - everysec：每秒同步一次
    - no：让操作系统决定何时同步

### Redis事务

Redis通过MULTI，EXEC，WATCH来实现事务功能。

- MULTI：开始事务
- EXEC：提交事务
- WATCH：监听事务

Redis不支持回滚

### 缓存异常

#### 缓存穿透

大量请求的数据不存在缓存（和数据库）中，导致大量请求直接落到数据库中。

解决方法：

- 布隆过滤器：将所有可能存在的数据用bitmap存起来，对所有请求的数据进行拦截，不存在的数据拦截掉
- 接口加校验，拦截非法数据

#### 缓存雪崩

缓存雪崩是指缓存在同一时间大面积失效，所有请求落在数据库上

解决方法：

- 设置随机的过期时间，避免同一时间大量数据过期
- 并发量不是特别多的时候，加锁排队
- 给每一个缓存数据增加相应的缓存标记，记录缓存是否失效，如果失效，更新数据缓存

### 如何保证缓存数据和数据库数据的一致性

有以下一些保证数据一致性的模式可用，可以在配置文件 application.yml中进行配置：

1. Cache-Aside模式

   Cache-Aside模式是一种简单且可靠的数据一致性方案，它通过在读取数据时先从缓存中获取数据，如果缓存中不存在，则从数据库中获取，并将获取的数据添加到缓存中。在写入数据时，先将数据写入数据库，然后再将缓存中的数据删除，以便下次读取时重新从数据库中获取。

2. Read-Through模式

   Read-Through模式是一种在缓存中缓存读取的数据，如果缓存中不存在，则从数据库中读取数据并将其添加到缓存中。在此模式下，应用程序只需从缓存中读取数据，而不必直接访问数据库。这种方式可以大大降低数据库负载，提高应用程序性能。

3. Write-Through模式

   Write-Through模式是一种将数据写入缓存和数据库的方案。在此模式下，应用程序首先将数据写入缓存中，然后再将数据写入数据库。如果写入数据库失败，将不会写入缓存。这样可以确保数据库和缓存中的数据一致性。

4. Write-Back模式

   Write-Back模式是一种先将数据写入缓存，然后异步将数据写入数据库的方案。在此模式下，应用程序将数据写入缓存中，然后标记缓存数据为“脏数据”。缓存服务异步地将“脏数据”写入数据库，以确保数据一致性。但是，Write-Back模式存在数据丢失的风险，因为如果缓存服务器在数据写入数据库之前崩溃，数据可能会丢失。

# MongoDB

| 数据库       | MongoDB                                              | MySQL                    |
| ------------ | ---------------------------------------------------- | ------------------------ |
| 数据库模型   | 非关系型                                             | 关系型                   |
| 存储方式     | 以类JSON的文档的格式存储                             | 不同引擎有不同的存储方式 |
| 查询语句     | MongoDB查询方式（类似JavaScript的函数）              | SQL语句                  |
| 数据处理方式 | 基于内存，将热数据存放在物理内存中，从而达到高速读写 | 不同引擎有自己的特点     |
| 事务性       | 仅支持单文档事务操作，弱一致性                       | 支持事务操作             |
| 占用空间     | 占用空间大                                           | 占用空间小               |
| join操作     | MongoDB没有join                                      | MySQL支持join            |
| 索引数据解构 | B树                                                  | B+树                     |
| 范围查询速度 | 慢                                                   | 快                       |
| 最大连接数   | 20000                                                | 100000                   |

### MapReduce

在 MongoDB 中，MapReduce 是一种用于处理大量数据的分布式计算方法。MapReduce 的核心思想是将大量数据分为若干个数据块，然后对每个数据块进行 Map 操作和 Reduce 操作，最终将所有 Map 和 Reduce 的结果合并起来得到最终的结果。

下面是 MapReduce 的基本步骤：

1. Map 阶段：将输入数据集合按照某种规则分为若干个数据块，并对每个数据块进行 Map 操作。Map 操作的结果是一组键值对，其中键表示某个属性的值，值表示该属性值出现的次数。
2. Shuffle 阶段：将 Map 操作的结果按照键进行分组，得到若干个键值对组。每个键值对组包含一个键和一个值列表。
3. Reduce 阶段：对每个键值对组进行 Reduce 操作，将该组中的值列表作为输入，得到一个输出结果。
4. 输出阶段：将所有 Reduce 的输出结果合并起来，得到最终的结果。

一般来说，我们只需要实现Map函数和Reduce函数，Shuffle阶段是由MapReduce引擎为我们自动完成的。
