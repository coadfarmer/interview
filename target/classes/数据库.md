# MYSQL

### 数据库三大范式

1. 第一范式：每个列不可拆分
2. 第二范式：在第一范式的基础上，非主键列都完全依赖主键，而不是依赖主键的一部分
3. 第三范式：在第二范式的基础上，非主键列只依赖于主键，不依赖于其他非主键

### MYSQL有哪些常用数据类型

1. 整数类型

   包括TINYINT、SMALLINT、MEDIUMINT、INT、BIGINT，分别表示1字节、2字节、3字节、4字节、8字节整数。任何整数类型都可以加上UNSIGNED属性，表示数据是无符号的，即非负整数。

2. 实数类型

   包括FLOAT、DOUBLE、DECIMAL（小数精确）。

3. 字符串类型

   char（定长）varchar（不定长）

   **注意**：

   对于经常变更的数据来说，CHAR比VARCHAR更好，因为CHAR不容易产生碎片。 对于非常短的列，CHAR比VARCHAR在存储空间上更有效率。 使用时要注意只分配需要的空间，更长的列排序时会消耗更多内存。
   尽量避免使用TEXT/BLOB类型，查询时会使用临时表，导致严重的性能开销。

4. 枚举类型：ENUM

5. 时间类型

   timestamp/datetime

### B+树

![B+tree](http://imysql.com/wp-content/uploads/2014/09/B-tree.png)

B+树，和B树一样，也是一种平衡的多叉查找树，不同之处在于：

- B树内部节点保存数据而B+树内部节点不保存数据，只做索引作用，它的叶子节点才保存数据
- B+树相邻的叶子节点之间通过链表指针连接起来

### MyISAM和InnoDB的区别

- myisam是非聚簇索引，innodb是聚簇索引
- MyISAM叶子节点存储的是数据地址，需要再寻址一次才能查询到数据
- Innodb的主键索引的叶子节点存储行数据，因此主键索引非常高效
- InnoDB非主键索引的叶子节点存储的是主键和其他索引的列数据，因此查询时做到覆盖索引会非常高效

### 什么是索引

通俗地说，索引就是目录。在数据库系统中，索引是一种排好序的数据结构，它包含数据表里所有记录的指针。索引的实现通常是B+树。

### 索引有哪些优缺点

- 优点
    - 可以大大加快数据的检索速度
- 缺点
    - 在增删改的时候，索引需要动态维护，降低了增删改的执行效率
    - 索引需要占用物理空间

### 索引使用场景

- where
- order by
- group by
- join
- distinct

### 索引有哪几种类型

- 主键索引

- 唯一索引

- 普通索引

  > 以上三种都属于B-TREE索引

- 全文索引

### 索引的算法和数据结构

B+树和hash，相对应的是B+树算法和hash算法。

B+树是一种多叉平衡树，数据保存在叶子中（B树是将信息保存在所有节点中），并且叶子节点上都有两个指针，分别指向上一个节点和下一个节点，组成一个双向链表。（一个节点就是1页，innoDB引擎存储是以页为单位的）

Hash算法，Hash碰撞的数据组成一个链表

### 创建索引的原则

1. 针对经常用于查询的列进行索引：应该为经常使用 `WHERE`、`ORDER BY` 、`GROUP BY` 、`DISTINCT`、`JOIN` 的列创建索引，这些列可以提高查询的速度。
2. 尽量选择小列创建索引：索引列的大小越小，索引的创建和维护就越快。
3. 不要在太多的列上创建索引：过多的索引会导致更新操作变得缓慢，也会增加磁盘空间的开销。
4. 尽量使用前缀索引：如果索引列的长度较长，可以使用前缀索引，即只索引列值的前面几个字符，以减小索引的大小。
5. 对于经常变更的列不要创建索引：由于索引会增加写操作的开销，因此对于经常变更的列不应该创建索引，例如日志表等。
6. 尽量少使用 `NULL` 值：在使用复合索引时，应该避免使用含有 `NULL` 值的列，因为 `NULL` 值需要额外的存储空间。

### 事务的四大特性

1. 原子性（Atomicity）：事务是最小的执行单位，要么全成功，要么全失败；
2. 一致性（Consistency）：事务执行前后，数据库中的数据应该保持一致性状态。如果事务执行过程中出现错误，所有已经执行的操作都会回滚，数据恢复到事务开始之前的状态。
3. 隔离性（Isolation）：多个事务并发执行时，每个事务都应该感觉不到其他事务的存在，每个事务都有独立的空间和资源，互不干扰。事务隔离级别越高，隔离性越好，但并发性能越差。
4. 持久性（Durability）：一旦事务提交，数据就会被永久保存到数据库中，即使系统崩溃也不会丢失。

### 什么是脏读，不可重复读，幻读

- 脏读：当一个事务正在访问数据并且对数据进行了修改，但是尚未提交时，另外一个事务也访问了这个数据，此时第二个事务读取到的是尚未提交的数据，这种现象被称为脏读。
- 不可重复读：当一个事务正在访问并读取数据时，另一个事务也访问了这个数据并进行了修改并提交，此时第一个事务再次读取到相同的数据，结果却不同，这种现象被称为不可重复读。
- 幻读：当一个事务在读取一定范围内的数据时，另外一个事务在该范围内插入了新的数据，第一个事务再次读取该范围内的数据时，发现数据比第一次读取的结果多了几条，这种现象被称为幻读。

### 什么是事务的隔离级别

| 隔离级别                     | 脏读 | 不可重复读 | 幻读 |
| ---------------------------- | ---- | ---------- | ---- |
| READ-UNCOMMITTED（读未提交） | √    | √          | √    |
| READ-COMMITTED（读已提交）   | ×    | √          | √    |
| REPEATABLE-READ（可重复读）  | ×    | ×          | √    |
| SERIALIZABLE（串行化）       | ×    | ×          | ×    |

MySQL默认采用REPEATABLE-READ（可重复读）隔离级别，Oracle默认采用READ_COMMITTED隔离级别

事务隔离机制基于锁机制和并发调度，并发调度使用的是MVVC

### MYSQL锁机制

#### 隔离级别与锁的关系

| 隔离级别                     | 锁                                           |
| ---------------------------- | -------------------------------------------- |
| READ-UNCOMMITTED（读未提交） | 读数据不加共享锁                             |
| READ-COMMITTED（读已提交）   | 读数据加共享锁，读完后释放共享锁             |
| REPEATABLE-READ（可重复读）  | 读数据加共享锁，事务提交后释放共享锁         |
| SERIALIZABLE（串行化）       | 锁定整个范围的键，并一直持有锁，直到事务完成 |

#### 按照锁粒度分为哪些锁

- 行级锁（InnoDB默认）
- 表级锁
- 页级锁

#### 按照锁类别分为哪些锁

- 共享锁：也就是读锁，用户读数据时，对数据加上共享锁
- 排他锁：写锁

#### 会发生死锁吗？怎么解决

会，当两个或多个事务在同一资源上相互占用，并请求锁定对方的资源，就会造成死锁。

解决方法：

1. 如果不同程序会并发存取多个表，尽量以相同的顺序访问表。
2. 在同一事务中，尽量做到一次锁定所需要的所有资源，减少死锁产生的概率。
3. 可以尝试升级锁粒度
4. 可以尝试使用分布式事务锁或乐观锁

#### MYSQL关联查询类型

1. 内连接
2. 外连接
3. 联合查询（union）：联合查询结果是将多个select语句的查询结果联合到一起。
4. 笛卡尔查询（cross join）

### EXPLAIN命令

- select_type:查询类型，simple、primary、union等
- **type**：访问类型：all，index，range，ref，eq_ref，const，system，NULL 越后面性能越好，《阿里巴巴Java开发手册》中推荐至少为range级别（索引范围查找）
- key：实际使用的索引
- extra：using index，using where

### 其他优化命令

- ANALYZE：分析表，如ANALYZE TABLE user;

- OPTIMIZE：优化表，如OPTIMIZE [LOCAL|NO_WRITE_TO_BINLOG] TABLE user;

  LOCAL|NO_WRITE_TO_BINLOG都是表示不写入日志.,优化表只对VARCHAR,BLOB和TEXT有效,通过OPTIMIZE TABLE语句可以消除文件碎片,在执行过程中会加上只读锁.

### 大表数据查询，怎么优化

1. 加索引
2. 加缓存
3. 读写分离
4. 分库分表

> [面对MySQL千万级别大表，你要如何优化？ | HeapDump性能社区](https://heapdump.cn/article/3850295)

#### 超大分页怎么处理

1. sql语句层面，比如取一百万行后的10条数据，mysql实际操作会取一百零十万行，然后截掉前100万行。这样会非常影响性能。

   比如一条语句 select * from table where 条件 limit 1000000,10;可以改为：

   select * from table where id in (select id from table where 条件 limit 1000000,10);

   更好的写法为：

   select * from table a,(select id from table where 条件 limit 100000,10) b where a.id = b.id

2. 加缓存

### 慢SQL问题如何解决

- #### SQL语句层面

    1. 使用索引：索引是提高 SQL 查询性能的常用手段，可以让查询更快速、更准确。通过分析查询语句及表结构，可以确定需要创建的索引类型、索引字段和索引顺序，来加速查询速度。
    2. 优化查询语句：对于复杂的 SQL 查询语句，可以进行重构，简化语句结构，优化子查询，避免使用多余的 JOIN 操作，尽可能利用数据库查询优化器来优化执行计划。

- #### 表层面

    1. 分析表结构：查看表结构，优化表结构设计，尽量避免使用大量的 BLOB 或 TEXT 类型的列，优化列类型、列长度、列默认值等。
    2. 水平拆分表：将一张表按照某个业务属性进行拆分，将大表拆成小表，避免单个表过大导致查询缓慢。
    3. 垂直拆分表：将一个大表按照列属性进行拆分，将一张表按照业务属性和数据特征进行垂直拆分，可以提高查询效率。
    
- #### 架构层面

    1. 加缓存：通过将查询结果缓存到缓存系统中，避免频繁访问数据库，降低数据库负载，提高查询效率。
    2. 优化服务器配置：可以通过优化服务器参数，例如调整缓存大小、调整 CPU 与内存的配置等，来提高系统整体性能。
    3. 数据库分片：将数据按照某个规则拆分到多个数据库节点上，提高系统的扩展性和性能。

### 什么是MVCC

MVCC是Multi-Version Concurrency Control的缩写，即多版本并发控制，是数据库系统中一种常见的并发控制机制。在数据库系统中，由于数据被多个用户同时访问和修改，因此需要一种机制来保证数据的一致性，而MVCC正是为此而生。

MVCC是通过在每行数据上创建多个版本来实现的，每个版本对应一个时间戳，不同的事务读取到的是不同的版本。在这种机制下，读操作不会阻塞写操作，而写操作也不会阻塞读操作，因此可以提高数据库的并发性能。同时，MVCC还可以实现非锁定的读操作，避免了锁定的开销，因此可以减少数据库的资源消耗。

### MYSQL日志

MYSQL日志主要包括错误日志，查询日志，慢查询日志，事务日志，二进制日志等几大类。其中比较重要的是binlog（二进制日志）、redo log（事务日志）和undo log（回滚日志）。

### 数据库三大日志

1. **binlog**（二进制日志）：记录对数据库的更新操作，例如增删改表，增删改数据等。记录的是SQL语句的二进制格式，可以用来实现数据的复制、恢复以及灾备等功能。
2. **redo log**（重做日志）：记录每个数据页在磁盘上的修改操作，包括物理页号、修改的偏移量、修改前和修改后的值等。主要是为了恢复数据库在发生异常（如宕机）时，未来得及写入到磁盘的数据。MySQL使用循环写的方式记录redo log，当redo log写满后，会回到开头重新写，覆盖之前的日志。
3. **undo log**（回滚日志）：记录事务中发生的修改操作，以便在回滚事务时撤销这些操作。undo log在事务提交时，会把修改操作应用到磁盘数据页上。在事务回滚时，会读取undo log，将之前修改的数据页还原回去。

# Redis

### 简单介绍下redis

Redis是一个由C语言开发的数据库，Redis的数据是存在内存中的，所以读写速度非常快。

Redis一般被用作缓存数据库，分布式锁，消息队列。

Redis的所有操作都是原子性的

Redis支持事务，持久化，集群。

### Redis除了做缓存，还能做什么

- 分布式锁——SETNX、Redisson
- 计数器：Redis的INCR/DECR命令可以实现计数器功能，可以用来实现网站的访问量统计、用户粉丝数量等计数功能。
- 消息队列
- 排行榜：利用Redis的有序集合和计数器功能，可以实现排行榜的实时更新和查询。

### Redis分布式锁有哪几种类型？

### Redis数据类型

Redis支持五大数据类型：String，Hash，List，Set，ZSet

- String：Redis最基本的数据类型，一个key对应一个value，最大存储512MB。

- Hash：一个key对应一个HashMap

- List：一个key对应一个List

- Set：一个key对应一个Set，通过Hash表实现

- ZSet（Sorted Set）：和Set的区别是ZSet是一个排好序的Set

  ```bash
  redis 127.0.0.1:6379> ZADD runoobkey 1 redis
  (integer) 1
  redis 127.0.0.1:6379> ZADD runoobkey 2 mongodb
  (integer) 1
  redis 127.0.0.1:6379> ZADD runoobkey 3 mysql
  (integer) 1
  redis 127.0.0.1:6379> ZADD runoobkey 3 mysql
  (integer) 0
  redis 127.0.0.1:6379> ZADD runoobkey 4 mysql
  (integer) 0
  redis 127.0.0.1:6379> ZRANGE runoobkey 0 10 WITHSCORES
  
  1) "redis"
  2) "1"
  3) "mongodb"
  4) "2"
  5) "mysql"
  6) "4"
  ```

- bitmap：位图，占空间小，只有0和1

### Redis持久化

#### 什么是Redis持久化

Redis持久化就是把内存中的数据写入磁盘中去

#### Redis持久化的机制是什么？有什么优缺点

Redis提供两种持久化机制：RDB和AOF机制

- RDB（Redis DataBase）

  RDB是Redis的默认持久化机制。它通过将Redis内存中的数据快照写入磁盘，将Redis的数据持久化到磁盘上。RDB持久化可以通过设置定期保存和手动触发保存两种方式进行。

  优点：

    - RDB持久化机制简单，适合于大规模的数据备份和恢复；
  - RDB持久化对Redis的性能影响相对较小，适合于IO密集型的应用场景。

  缺点：

    - RDB持久化机制需要将整个Redis数据集写入磁盘，因此如果数据量过大，会导致磁盘IO负载过高；
  - RDB持久化机制对于数据的实时性要求较低，如果Redis出现异常情况导致数据未能及时保存，可能会导致数据的丢失。

- AOF（Append only file）

  AOF持久化机制是Redis提供的另一种持久化机制，它通过将Redis执行的写命令追加到一个日志文件中，实现对Redis数据的持久化。

  优点：
  
  - AOF持久化机制可以保证数据的实时性，可以通过设置同步频率来控制数据的持久化频率；
  - AOF持久化机制对于数据的灵活性和可靠性更高，可以保证更多的数据不会丢失。
  
  缺点：
  
  - AOF持久化机制需要将Redis执行的写命令持久化到磁盘上，因此相对于RDB持久化机制，AOF持久化机制对Redis的性能影响较大；
  - AOF持久化机制的文件体积可能比RDB持久化机制更大，需要定期进行压缩和重写。

### Redis事务

Redis通过MULTI，EXEC，WATCH来实现事务功能。

- MULTI：开启一个事务。
- EXEC：提交事务，执行所有事务队列中的命令。
- DISCARD：取消事务，清空当前事务队列中的所有命令。
- WATCH：监视一个或多个键，当这些键被其他客户端修改时，当前事务会被中断。

Redis不支持回滚

### 缓存异常

#### 缓存穿透

大量请求的数据不存在缓存和数据库中，导致大量请求直接落到数据库中。

解决方法：
1. 缓存空对象

   当缓存查询结果为空时，将空对象也缓存起来，并设置较短的过期时间。这样，在下一次查询时，如果查询的是同一个空对象，就可以直接从缓存中获取，而不需要去访问数据库。

2. 布隆过滤器

   布隆过滤器可以用于检测请求的参数是否合法。当请求的参数经过布隆过滤器的检测后，如果被认为是无效的，可以直接返回，而不需要去访问数据库。

3. 缓存预热

   在应用启动时，将常用的数据预先加载到缓存中，可以有效减少缓存穿透的发生率。

4. 限流

   当应用接收到大量请求时，可以采取限流策略，例如设置访问频率限制或者访问并发数限制，以减轻数据库压力。

#### 缓存雪崩

缓存雪崩是指当缓存中大量的数据同时失效或者缓存服务不可用时，请求将直接访问数据库，导致数据库瞬间承受了过大的压力，进而影响整个系统的稳定性和可用性。

缓存雪崩通常是由于缓存中大量数据在同一时刻过期或者缓存服务宕机导致的。当缓存中大量的数据同时失效时，所有的请求都需要直接访问数据库，导致数据库压力瞬间增大，从而导致系统崩溃。同样地，如果缓存服务宕机，也会导致类似的问题。

解决方法：

1. 缓存数据的过期时间设置随机

   将缓存中数据的过期时间设置为随机时间，可以有效避免大量数据同时过期的情况。

2. 加入缓存数据的热点监控

   监控系统可以对缓存中的热点数据进行监控，当热点数据的缓存失效时，及时进行补充，以避免缓存雪崩的发生。

3. 多级缓存架构

   通过多级缓存架构，将缓存分为多个层级，可以有效减少缓存失效的风险。在多级缓存架构中，高级别的缓存可以缓存较为常用的数据，低级别的缓存可以缓存较不常用的数据。这样，即使高级别的缓存出现了问题，低级别的缓存依然可以保证系统的稳定性。

4. 数据预热

   在系统启动时，预先加载缓存中的数据，可以有效减少缓存失效的风险。

### 如何保证缓存数据和数据库数据的一致性

有以下一些保证数据一致性的模式可用，可以在配置文件 application.yml中进行配置：

1. Cache-Aside模式

   Cache-Aside模式是一种简单且可靠的数据一致性方案，它通过在读取数据时先从缓存中获取数据，如果缓存中不存在，则从数据库中获取，并将获取的数据添加到缓存中。在写入数据时，先将数据写入数据库，然后再将缓存中的数据删除，以便下次读取时重新从数据库中获取。

2. Read-Through模式

   Read-Through模式是一种在缓存中缓存读取的数据，如果缓存中不存在，则从数据库中读取数据并将其添加到缓存中。在此模式下，应用程序只需从缓存中读取数据，而不必直接访问数据库。这种方式可以大大降低数据库负载，提高应用程序性能。

3. Write-Through模式

   Write-Through模式是一种将数据写入缓存和数据库的方案。在此模式下，应用程序首先将数据写入缓存中，然后再将数据写入数据库。如果写入数据库失败，将不会写入缓存。这样可以确保数据库和缓存中的数据一致性。

4. Write-Back模式

   Write-Back模式是一种先将数据写入缓存，然后异步将数据写入数据库的方案。在此模式下，应用程序将数据写入缓存中，然后标记缓存数据为“脏数据”。缓存服务异步地将“脏数据”写入数据库，以确保数据一致性。但是，Write-Back模式存在数据丢失的风险，因为如果缓存服务器在数据写入数据库之前崩溃，数据可能会丢失。

# MongoDB

| 数据库       | MongoDB                                              | MySQL                    |
| ------------ | ---------------------------------------------------- | ------------------------ |
| 数据库模型   | 非关系型                                             | 关系型                   |
| 存储方式     | 以类JSON的文档的格式存储                             | 不同引擎有不同的存储方式 |
| 查询语句     | MongoDB查询方式（类似JavaScript的函数）              | SQL语句                  |
| 数据处理方式 | 基于内存，将热数据存放在物理内存中，从而达到高速读写 | 不同引擎有自己的特点     |
| 事务性       | 仅支持单文档事务操作，弱一致性                       | 支持事务操作             |
| 占用空间     | 占用空间大                                           | 占用空间小               |
| join操作     | MongoDB没有join                                      | MySQL支持join            |
| 索引数据解构 | B树                                                  | B+树                     |
| 范围查询速度 | 慢                                                   | 快                       |
| 最大连接数   | 20000                                                | 100000                   |

### MapReduce

在 MongoDB 中，MapReduce 是一种用于处理大量数据的分布式计算方法。MapReduce 的核心思想是将大量数据分为若干个数据块，然后对每个数据块进行 Map 操作和 Reduce 操作，最终将所有 Map 和 Reduce 的结果合并起来得到最终的结果。

下面是 MapReduce 的基本步骤：

1. Map 阶段：将输入数据集合按照某种规则分为若干个数据块，并对每个数据块进行 Map 操作。Map 操作的结果是一组键值对，其中键表示某个属性的值，值表示该属性值出现的次数。
2. Shuffle 阶段：将 Map 操作的结果按照键进行分组，得到若干个键值对组。每个键值对组包含一个键和一个值列表。
3. Reduce 阶段：对每个键值对组进行 Reduce 操作，将该组中的值列表作为输入，得到一个输出结果。
4. 输出阶段：将所有 Reduce 的输出结果合并起来，得到最终的结果。

一般来说，我们只需要实现Map函数和Reduce函数，Shuffle阶段是由MapReduce引擎为我们自动完成的。
